{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEK DATA PALING SEDIKIT BERAPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1_data_analysis.csv: 17865 rows\n",
      "1-2_data_analysis.csv: 18994 rows\n",
      "1-3_data_analysis.csv: 17732 rows\n",
      "10-1_data_analysis.csv: 17866 rows\n",
      "10-3_data_analysis.csv: 17889 rows\n",
      "11-1_data_analysis.csv: 17914 rows\n",
      "11-2_data_analysis.csv: 17886 rows\n",
      "11-3_data_analysis.csv: 17875 rows\n",
      "12-1_data_analysis.csv: 17889 rows\n",
      "13-1_data_analysis.csv: 17902 rows\n",
      "13-2_data_analysis.csv: 17900 rows\n",
      "14-1_data_analysis.csv: 17883 rows\n",
      "14-2_data_analysis.csv: 17861 rows\n",
      "14-3_data_analysis.csv: 17918 rows\n",
      "2-1_data_analysis.csv: 17899 rows\n",
      "2-2_data_analysis.csv: 18802 rows\n",
      "2-3_data_analysis.csv: 16292 rows\n",
      "3-1_data_analysis.csv: 17882 rows\n",
      "3-2_data_analysis.csv: 17696 rows\n",
      "3-3_data_analysis.csv: 17748 rows\n",
      "4-1_data_analysis.csv: 17789 rows\n",
      "4-2_data_analysis.csv: 16926 rows\n",
      "4-3_data_analysis.csv: 17791 rows\n",
      "5-1_data_analysis.csv: 17919 rows\n",
      "5-2_data_analysis.csv: 16350 rows\n",
      "5-3_data_analysis.csv: 16460 rows\n",
      "6-1_data_analysis.csv: 17913 rows\n",
      "6-2_data_analysis.csv: 17704 rows\n",
      "6-3_data_analysis.csv: 17914 rows\n",
      "7-2_data_analysis.csv: 15824 rows\n",
      "7-3_data_analysis.csv: 17756 rows\n",
      "8-1_data_analysis.csv: 17863 rows\n",
      "8-2_data_analysis.csv: 17748 rows\n",
      "8-3_data_analysis.csv: 18050 rows\n",
      "9-2_data_analysis.csv: 17908 rows\n",
      "9-3_data_analysis.csv: 17922 rows\n",
      "Total rows across all CSV files: 637530\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "directory = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\Augmented\\dataEARMAR\\Normal'\n",
    "\n",
    "# Initialize a variable to store the total number of rows\n",
    "total_rows = 0\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Get the number of rows in the CSV file\n",
    "        num_rows = len(df)\n",
    "        \n",
    "        # Print the number of rows in the current file\n",
    "        print(f'{filename}: {num_rows} rows')\n",
    "        \n",
    "        # Add the number of rows to the total count\n",
    "        total_rows += num_rows\n",
    "\n",
    "# Print the total number of rows across all CSV files\n",
    "print(f'Total rows across all CSV files: {total_rows}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMAKAN PANJANG DATA PADA SEMUA CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1_2_ready_output.csv and saved to F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test\\output\\1_2_ready_output.csv\n",
      "Processed 2_3_ready_output.csv and saved to F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test\\output\\2_3_ready_output.csv\n",
      "All files have been processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Tentukan direktori yang berisi file CSV\n",
    "directory = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test'\n",
    "\n",
    "output_directory = os.path.join(directory, 'output')\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Tentukan jumlah baris yang akan diambil dari atas dan bawah\n",
    "num_rows_top = 56\n",
    "num_rows_bottom = 56\n",
    "\n",
    "# Iterasi melalui setiap file CSV di dalam direktori\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_head = df.head(num_rows_top)\n",
    "        df_tail = df.tail(num_rows_bottom)\n",
    "        df_combined = pd.concat([df_head, df_tail])\n",
    "        \n",
    "        # Simpan ke file CSV baru\n",
    "        output_file_path = os.path.join(output_directory, f'{filename}')\n",
    "        df_combined.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        print(f'Processed {filename} and saved to {output_file_path}')\n",
    "\n",
    "print(\"All files have been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test\\output\\PERCLOSready.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Tentukan direktori yang berisi file CSV\n",
    "directory = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test\\output'\n",
    "\n",
    "# Daftar label kelas yang sesuai dengan gambar yang Anda kirimkan\n",
    "class_labels = [\n",
    "    1\n",
    "]\n",
    "\n",
    "# Buat list untuk menyimpan data yang digabungkan\n",
    "combined_data = []\n",
    "\n",
    "# Iterasi melalui setiap file CSV di dalam direktori\n",
    "for idx, filename in enumerate(sorted(os.listdir(directory))):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Baca data CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Gabungkan data dari kolom PERCLOS dan MAR_Std menjadi satu baris\n",
    "        combined_row = {\n",
    "            'PERCLOS': df['PERCLOS'].tolist(),\n",
    "            'MAR_Std': df['MAR_Std'].tolist(),\n",
    "            'Class': class_labels[idx]\n",
    "        }\n",
    "        \n",
    "        # Tambahkan baris ke list combined_data\n",
    "        combined_data.append(combined_row)\n",
    "    \n",
    "\n",
    "# Buat DataFrame dari combined_data\n",
    "df_combined = pd.DataFrame(combined_data)\n",
    "\n",
    "# Simpan data gabungan ke file CSV baru\n",
    "output_file_path = os.path.join(directory, 'PERCLOSready.csv')\n",
    "df_combined.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f'Combined data saved to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perbanyak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class distribution:\n",
      "1    15\n",
      "2    15\n",
      "3    15\n",
      "Name: Class, dtype: int64\n",
      "   PERCLOS_1  PERCLOS_2  PERCLOS_3  PERCLOS_4  PERCLOS_5  PERCLOS_6  \\\n",
      "0   0.043419   0.045798   0.036318   0.059089   0.040214   0.060469   \n",
      "1   0.040074   0.097299   0.054799   0.092829   0.088078   0.237182   \n",
      "2   0.041241   0.036259   0.050221   0.198348   0.055240   0.159130   \n",
      "3   0.021326   0.083385   0.165688   0.066084   0.062095   0.196509   \n",
      "4   0.063602   0.039595   0.063575   0.043380   0.042795   0.118611   \n",
      "\n",
      "   PERCLOS_7  PERCLOS_8  PERCLOS_9  PERCLOS_10  ...  MAR_Std_104  MAR_Std_105  \\\n",
      "0   0.041843   0.036710   0.046022    0.105875  ...     0.021216     0.014352   \n",
      "1   0.042022   0.040904   0.096316    0.047359  ...     0.014963     0.011904   \n",
      "2   0.053408   0.049147   0.044983    0.150187  ...     0.018810     0.012747   \n",
      "3   0.077532   0.023084   0.105114    0.200760  ...     0.016263     0.014681   \n",
      "4   0.044477   0.141544   0.207123    0.148512  ...     0.016786     0.019836   \n",
      "\n",
      "   MAR_Std_106  MAR_Std_107  MAR_Std_108  MAR_Std_109  MAR_Std_110  \\\n",
      "0     0.018649     0.020571     0.018085     0.026470     0.027396   \n",
      "1     0.018734     0.012874     0.016917     0.015245     0.018995   \n",
      "2     0.015701     0.021801     0.014972     0.011884     0.015728   \n",
      "3     0.019566     0.018272     0.009221     0.015207     0.007399   \n",
      "4     0.024306     0.017025     0.015859     0.015398     0.014391   \n",
      "\n",
      "   MAR_Std_111  MAR_Std_112  Class  \n",
      "0     0.021417     0.026166      1  \n",
      "1     0.019772     0.015170      2  \n",
      "2     0.013899     0.017060      3  \n",
      "3     0.017328     0.018665      1  \n",
      "4     0.019452     0.017693      3  \n",
      "\n",
      "[5 rows x 225 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "file_path = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\PerclosEAR\\output\\PERCLOSready.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Flatten the lists in 'PERCLOS' and 'MAR_Std' and create a new dataframe\n",
    "perclos_flattened = data['PERCLOS'].apply(eval).apply(np.array)\n",
    "mar_std_flattened = data['MAR_Std'].apply(eval).apply(np.array)\n",
    "\n",
    "# Creating new dataframe with flattened columns\n",
    "flat_data = pd.DataFrame({\n",
    "    'PERCLOS': list(perclos_flattened),\n",
    "    'MAR_Std': list(mar_std_flattened),\n",
    "    'Class': data['Class']\n",
    "})\n",
    "\n",
    "# Expanding the arrays into separate columns\n",
    "perclos_df = pd.DataFrame(flat_data['PERCLOS'].tolist(), columns=[f'PERCLOS_{i+1}' for i in range(flat_data['PERCLOS'].apply(len).max())])\n",
    "mar_std_df = pd.DataFrame(flat_data['MAR_Std'].tolist(), columns=[f'MAR_Std_{i+1}' for i in range(flat_data['MAR_Std'].apply(len).max())])\n",
    "\n",
    "# Concatenate expanded data with the 'Class' column\n",
    "expanded_data = pd.concat([perclos_df, mar_std_df, flat_data['Class']], axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = expanded_data.drop(columns=['Class'])\n",
    "y = expanded_data['Class']\n",
    "\n",
    "# Apply SMOTE to the data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Concatenate the resampled features and target\n",
    "resampled_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class'])], axis=1)\n",
    "\n",
    "# Check the final distribution of the classes\n",
    "final_class_distribution = resampled_data['Class'].value_counts()\n",
    "print(\"Final class distribution:\")\n",
    "print(final_class_distribution)\n",
    "\n",
    "# Save the resampled data to a CSV file\n",
    "output_file_path = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\PerclosEAR\\output\\PERCLOSready_resampled1.csv'\n",
    "resampled_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the resampled data\n",
    "print(resampled_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kembalikan Ke bentuk Siap Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_data = pd.read_csv(r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\PerclosEAR\\output\\PERCLOSready_resampled1.csv')\n",
    "perclos_columns = original_data.filter(like='PERCLOS').columns\n",
    "mar_std_columns = original_data.filter(like='MAR_Std').columns\n",
    "\n",
    "perclos_values = original_data[perclos_columns].values.tolist()\n",
    "mar_std_values = original_data[mar_std_columns].values.tolist()\n",
    "corrected_data = pd.DataFrame({\n",
    "    'PERCLOS': perclos_values,\n",
    "    'MAR_Std': mar_std_values,\n",
    "    'Class': original_data['Class']\n",
    "})\n",
    "\n",
    "\n",
    "corrected_data.to_csv(r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\PerclosEAR\\output\\PERCLOSreadySmote_fix.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS UNTUK TESTING MODEL (DATA NYA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1_2_ready_output.csv and saved to F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test\\output\\1_2_ready_output.csv\n",
      "All files have been processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Tentukan direktori yang berisi file CSV\n",
    "directory = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test'\n",
    "\n",
    "output_directory = os.path.join(directory, 'output')\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Tentukan jumlah baris yang akan diambil dari atas dan bawah\n",
    "num_rows_top = 56\n",
    "num_rows_bottom = 56\n",
    "\n",
    "# Iterasi melalui setiap file CSV di dalam direktori\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_head = df.head(num_rows_top)\n",
    "        df_tail = df.tail(num_rows_bottom)\n",
    "        df_combined = pd.concat([df_head, df_tail])\n",
    "        \n",
    "        # Simpan ke file CSV baru\n",
    "        output_file_path = os.path.join(output_directory, f'{filename}')\n",
    "        df_combined.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        print(f'Processed {filename} and saved to {output_file_path}')\n",
    "\n",
    "print(\"All files have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test\\output\\PERCLOSready.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Tentukan direktori yang berisi file CSV\n",
    "directory = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\test\\output'\n",
    "\n",
    "# # Daftar label kelas yang sesuai dengan gambar yang Anda kirimkan\n",
    "class_labels = [\n",
    "    1\n",
    "]\n",
    "\n",
    "# Buat list untuk menyimpan data yang digabungkan\n",
    "combined_data = []\n",
    "\n",
    "# Iterasi melalui setiap file CSV di dalam direktori\n",
    "for idx, filename in enumerate(sorted(os.listdir(directory))):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Baca data CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Gabungkan data dari kolom PERCLOS dan MAR_Std menjadi satu baris\n",
    "        combined_row = {\n",
    "            'PERCLOS': df['PERCLOS'].tolist(),\n",
    "            'MAR_Std': df['MAR_Std'].tolist(),\n",
    "            'Class': class_labels[idx]\n",
    "        }\n",
    "        \n",
    "        # Tambahkan baris ke list combined_data\n",
    "        combined_data.append(combined_row)\n",
    "    \n",
    "\n",
    "# Buat DataFrame dari combined_data\n",
    "df_combined = pd.DataFrame(combined_data)\n",
    "\n",
    "# Simpan data gabungan ke file CSV baru\n",
    "output_file_path = os.path.join(directory, 'PERCLOSready.csv')\n",
    "df_combined.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f'Combined data saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE UNTUK MEMBUAT DATA TAMBAHAN SETELAH SMOTE DISAMAKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class distribution:\n",
      "1    35\n",
      "2    35\n",
      "3    35\n",
      "Name: Class, dtype: int64\n",
      "   PERCLOS_1  PERCLOS_2  PERCLOS_3  PERCLOS_4  PERCLOS_5  PERCLOS_6  \\\n",
      "0   0.043419   0.045798   0.036318   0.059089   0.040214   0.060469   \n",
      "1   0.040074   0.097299   0.054799   0.092829   0.088078   0.237182   \n",
      "2   0.041241   0.036259   0.050221   0.198348   0.055240   0.159130   \n",
      "3   0.021326   0.083385   0.165688   0.066084   0.062095   0.196509   \n",
      "4   0.063602   0.039595   0.063575   0.043380   0.042795   0.118611   \n",
      "\n",
      "   PERCLOS_7  PERCLOS_8  PERCLOS_9  PERCLOS_10  ...  MAR_Std_104  MAR_Std_105  \\\n",
      "0   0.041843   0.036710   0.046022    0.105875  ...     0.021216     0.014352   \n",
      "1   0.042022   0.040904   0.096316    0.047359  ...     0.014963     0.011904   \n",
      "2   0.053408   0.049147   0.044983    0.150187  ...     0.018810     0.012747   \n",
      "3   0.077532   0.023084   0.105114    0.200760  ...     0.016263     0.014681   \n",
      "4   0.044477   0.141544   0.207123    0.148512  ...     0.016786     0.019836   \n",
      "\n",
      "   MAR_Std_106  MAR_Std_107  MAR_Std_108  MAR_Std_109  MAR_Std_110  \\\n",
      "0     0.018649     0.020571     0.018085     0.026470     0.027396   \n",
      "1     0.018734     0.012874     0.016917     0.015245     0.018995   \n",
      "2     0.015701     0.021801     0.014972     0.011884     0.015728   \n",
      "3     0.019566     0.018272     0.009221     0.015207     0.007399   \n",
      "4     0.024306     0.017025     0.015859     0.015398     0.014391   \n",
      "\n",
      "   MAR_Std_111  MAR_Std_112  Class  \n",
      "0     0.021417     0.026166      1  \n",
      "1     0.019772     0.015170      2  \n",
      "2     0.013899     0.017060      3  \n",
      "3     0.017328     0.018665      1  \n",
      "4     0.019452     0.017693      3  \n",
      "\n",
      "[5 rows x 225 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "file_path = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\PerclosEAR\\output\\PERCLOSreadySmote_reshaped.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Flatten the lists in 'PERCLOS' and 'MAR_Std' and create a new dataframe\n",
    "perclos_flattened = data['PERCLOS'].apply(eval).apply(np.array)\n",
    "mar_std_flattened = data['MAR_Std'].apply(eval).apply(np.array)\n",
    "\n",
    "# Creating new dataframe with flattened columns\n",
    "flat_data = pd.DataFrame({\n",
    "    'PERCLOS': list(perclos_flattened),\n",
    "    'MAR_Std': list(mar_std_flattened),\n",
    "    'Class': data['Class']\n",
    "})\n",
    "\n",
    "# Expanding the arrays into separate columns\n",
    "perclos_df = pd.DataFrame(flat_data['PERCLOS'].tolist(), columns=[f'PERCLOS_{i+1}' for i in range(flat_data['PERCLOS'].apply(len).max())])\n",
    "mar_std_df = pd.DataFrame(flat_data['MAR_Std'].tolist(), columns=[f'MAR_Std_{i+1}' for i in range(flat_data['MAR_Std'].apply(len).max())])\n",
    "\n",
    "# Concatenate expanded data with the 'Class' column\n",
    "expanded_data = pd.concat([perclos_df, mar_std_df, flat_data['Class']], axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = expanded_data.drop(columns=['Class'])\n",
    "y = expanded_data['Class']\n",
    "\n",
    "# Function to apply SMOTE incrementally\n",
    "def incremental_smote(X, y, target_class, n_samples):\n",
    "    smote = SMOTE(sampling_strategy={target_class: len(y[y == target_class]) + n_samples}, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Incrementally apply SMOTE to add 10 samples for each class\n",
    "unique_classes = y.unique()\n",
    "X_resampled = X.copy()\n",
    "y_resampled = y.copy()\n",
    "\n",
    "for target_class in unique_classes:\n",
    "    X_resampled, y_resampled = incremental_smote(X_resampled, y_resampled, target_class, 20)\n",
    "\n",
    "# Concatenate the resampled features and target\n",
    "resampled_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class'])], axis=1)\n",
    "\n",
    "# Check the final distribution of the classes\n",
    "final_class_distribution = resampled_data['Class'].value_counts()\n",
    "print(\"Final class distribution:\")\n",
    "print(final_class_distribution)\n",
    "\n",
    "# Save the resampled data to a CSV file\n",
    "output_file_path = r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\PerclosEAR\\output\\PERCLOSready_resampled1.csv'\n",
    "resampled_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the resampled data\n",
    "print(resampled_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_data = pd.read_csv(r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\PerclosEAR\\output\\PERCLOSready_resampled1.csv')\n",
    "perclos_columns = original_data.filter(like='PERCLOS').columns\n",
    "mar_std_columns = original_data.filter(like='MAR_Std').columns\n",
    "\n",
    "perclos_values = original_data[perclos_columns].values.tolist()\n",
    "mar_std_values = original_data[mar_std_columns].values.tolist()\n",
    "corrected_data = pd.DataFrame({\n",
    "    'PERCLOS': perclos_values,\n",
    "    'MAR_Std': mar_std_values,\n",
    "    'Class': original_data['Class']\n",
    "})\n",
    "\n",
    "\n",
    "corrected_data.to_csv(r'F:\\SKRIPSI\\TA\\Fix_code\\SkripSHYs\\FixedDataset\\NonAugmented\\PerclosEAR\\output\\PERCLOSready_resampled1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Asus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
